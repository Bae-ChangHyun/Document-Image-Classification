{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"OliaDaX_lwou"},"source":["# **📄 Document type classification baseline code**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9396,"status":"ok","timestamp":1700314592802,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"3BaoIkv5Xwa0"},"outputs":[],"source":["import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import copy\n","import wandb\n","from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support, confusion_matrix\n","\n","import timm\n","import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.datasets import ImageFolder\n","from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n","import random\n","import torch.backends.cudnn as cudnn\n","from focal_loss.focal_loss import FocalLoss # https://github.com/mathiaszinnen/focal_loss_torch\n","\n","import cv2\n","from PIL import Image\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","# pip uninstall charset-normalizer\n","# pip install charset-normalizer\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 시드 고정\n","def random_seed(seed_num):\n","    torch.manual_seed(seed_num)\n","    torch.cuda.manual_seed(seed_num)\n","    torch.cuda.manual_seed_all(seed_num)\n","    np.random.seed(seed_num)\n","    cudnn.benchmark = False\n","    cudnn.deterministic = True\n","    random.seed(seed_num)\n","random_seed(624)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# class와 label이 매핑되어있는 파일 \n","meta_df = pd.read_csv('data/meta.csv')\n","meta_df=pd.read_csv('data/meta.csv')\n","label2id = dict(zip(meta_df['class_name'], meta_df['target']))\n","id2label = dict(zip(meta_df['target'], meta_df['class_name']))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1700314772722,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"Hyl8oAy6TZAu"},"outputs":[],"source":["# 커스텀 데이터 셋\n","class ImageDataset(Dataset):\n","    def __init__(self, csv, path, transform=None):\n","        self.df = pd.read_csv(csv).values\n","        self.path = path\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        name, target = self.df[idx]\n","        img = np.array(Image.open(os.path.join(self.path, name)))\n","        # 이미지가 흑색조인 경우 RGB로\n","        if len(img.shape) < 3 or img.shape[2] != 3:\n","            img = np.stack([img] * 3, axis=-1)\n","        if self.transform:\n","            img = self.transform(image=img)['image']\n","        return img, target,name"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://geunuk.tistory.com/456\n","def MixUp(input, target, alpha=1.0):\n","    if alpha > 0:\n","        lambda_ = np.random.beta(alpha, alpha)\n","    else:\n","        lambda_ = 1\n"," \n","    batch_size = input.size(0)\n","    index = torch.randperm(batch_size)\n","    \n","    mixed_input = lambda_ * input + (1 - lambda_) * input[index, :]    \n","    labels_a, labels_b = target, target[index]\n"," \n","    return mixed_input, labels_a, labels_b, lambda_\n","\n","def MixUpLoss(criterion, pred, labels_a, labels_b, lambda_):\n","    return lambda_ * criterion(pred, labels_a) + (1 - lambda_) * criterion(pred, labels_b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DataLoader 정의\n","def dataset(AUG_BOOL,trn_transform,tst_transform):\n","    \n","    # 기존 이미지 혹은 증강된 이미지\n","    if(AUG_BOOL):train_img_file=\"data/aug_train.csv\"\n","    else: train_img_file=\"data/train.csv\"\n","\n","    origin_train_dataset = ImageDataset(\n","        \"data/train.csv\",\n","        \"data/train/\",\n","        transform=trn_transform\n","    )\n","    trn_dataset = ImageDataset(\n","        train_img_file,  \n","        \"data/aug_train/\",\n","        transform=trn_transform\n","    )\n","    val_dataset = ImageDataset(\n","        \"data/aug_valid.csv\",\n","        \"data/aug_valid/\",\n","        transform=trn_transform\n","    )\n","    tst_dataset = ImageDataset(\n","        \"data/aug_test.csv\",\n","        \"data/aug_test/\",\n","        transform=trn_transform\n","    )\n","    origin_tst_dataset = ImageDataset(\n","        \"data/sample_submission.csv\",\n","        \"data/test/\",\n","        transform=tst_transform\n","    )\n","    \n","    return origin_train_dataset,trn_dataset,val_dataset,tst_dataset,origin_tst_dataset\n","\n","# 파라미터에 따른 데이터 로더\n","def loader(batch_size,origin_train_dataset,trn_dataset,val_dataset,tst_dataset,origin_tst_dataset):\n","    origin_train_loader = DataLoader(\n","        origin_train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    train_loader = DataLoader(\n","        trn_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    valid_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    test_loader = DataLoader(\n","        tst_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    origin_test_loader = DataLoader(\n","        origin_tst_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    \n","    return origin_train_loader,train_loader,valid_loader,test_loader,origin_test_loader\n","\n","# 이미지 사이즈에 따른 Transform (각 모델별 input 이미지 사이즈와 정규화)\n","def image_trasform(image_size,model_mean,model_std):\n","    trn_transform = A.Compose([\n","        A.Resize(height=image_size, width=image_size),\n","        A.Normalize(mean=list(model_mean), std=list(model_std)),\n","        ToTensorV2(),\n","        ])\n","    tst_transform = A.Compose([\n","        A.Resize(height=image_size, width=image_size),\n","        A.Normalize(mean=list(model_mean), std=list(model_std)),\n","        ToTensorV2(),\n","    ])\n","    \n","    return trn_transform,tst_transform"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":255,"status":"ok","timestamp":1700315066028,"user":{"displayName":"Ynot(송원호)","userId":"16271863862696372773"},"user_tz":-540},"id":"kTECBJfVTbdl"},"outputs":[],"source":["# training, evaluation, training_loop 코드\n","def training(model, dataloader, train_dataset, criterion, optimizer, device, epoch, EPOCH):\n","\n","  model.train()  # 모델을 학습 모드로 설정\n","  train_loss = 0.0\n","  train_accuracy = 0\n","  all_labels = []\n","  all_predicted = []\n"," \n","  tbar = tqdm(dataloader)\n","  for idx,(images, labels, names) in enumerate(tbar):\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      \n","      # Mixup 적용 배치가 3으로 나눠떨어질 떄 마다 실행\n","      if (idx + 1) % 3 == 0:\n","          images, labels_a, labels_b, lambda_ = MixUp(images, labels)\n","          outputs = model(images)\n","          if isinstance(outputs, torch.Tensor): outputs = outputs\n","          else: outputs = outputs.logits\n","          loss = MixUpLoss(criterion, pred=outputs, labels_a=labels_a, labels_b=labels_b, lambda_=lambda_)\n","      else:                    \n","          outputs = model(images)\n","          if isinstance(outputs, torch.Tensor): outputs = outputs\n","          else: outputs = outputs.logits\n","          if(LOSS_F=='CE'):loss = criterion(outputs, labels)\n","          else: loss = criterion(m(outputs), labels) # focal loss\n","  \n","      # 역전파 및 가중치 업데이트\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n"," \n","      # 손실과 정확도 계산\n","      train_loss += loss.item()\n","      \n","      # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n","      # _는 가장 높은 클래스 확률값, predicted는 가장 높은 클래스\n","      _, predicted = torch.max(outputs, 1)\n","      train_accuracy += (predicted == labels).sum().item()\n","      \n","      all_labels.extend(labels.cpu().numpy())\n","      all_predicted.extend(predicted.cpu().numpy())\n"," \n","      tbar.set_description(f\"Epoch [{epoch+1}/{EPOCH}], Train Loss: {loss.item():.4f}\")\n"," \n","  # 에폭별 학습 결과 출력\n","  train_loss = train_loss / len(dataloader)\n","  train_accuracy = train_accuracy / len(train_dataset)\n","  precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro')\n"," \n","  return model, train_loss, train_accuracy, f1\n"," \n","def evaluation(model, dataloader, val_dataset, criterion, device, epoch, EPOCH):\n","  model.eval()  # 모델을 평가 모드로 설정\n","  valid_loss = 0.0\n","  valid_accuracy = 0\n","  all_labels = []\n","  all_predicted = []\n"," \n","  with torch.no_grad(): # model의 업데이트 막기\n","      tbar = tqdm(dataloader)\n","      for images, labels, names in tbar:\n","          images = images.to(device)\n","          labels = labels.to(device)\n"," \n","          # 순전파\n","          outputs = model(images)\n","          # timm에서 불러온 모델과 automodel에서 불러온 모델의 output 형태가 다름\n","          if isinstance(outputs, torch.Tensor): outputs = outputs\n","          else: outputs = outputs.logits\n","          \n","          if(LOSS_F=='CE'):loss = criterion(outputs, labels)\n","          else: loss = criterion(m(outputs), labels) # focal loss\n"," \n","          # 손실과 정확도 계산\n","          valid_loss += loss.item()\n","          # torch.max에서 dim 인자에 값을 추가할 경우, 해당 dimension에서 최댓값과 최댓값에 해당하는 인덱스를 반환\n","          _, predicted = torch.max(outputs, 1)\n","          valid_accuracy += (predicted == labels).sum().item()\n","          \n","          all_labels.extend(labels.cpu().numpy())\n","          all_predicted.extend(predicted.cpu().numpy())\n"," \n","          tbar.set_description(f\"Epoch [{epoch+1}/{EPOCH}], Valid Loss: {loss.item():.4f}\")\n","          \n","  # 평가지수\n","  valid_loss = valid_loss / len(dataloader)\n","  valid_accuracy = valid_accuracy / len(val_dataset)\n","  precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predicted, average='macro') # zero_division=1\n"," \n","  return model, valid_loss, valid_accuracy, f1\n"," \n"," \n","def training_loop(model, train_loader,valid_loader,train_dataset, val_dataset, criterion, optimizer, scheduler, device, patience, EPOCH, lr_scheduler_name):\n","  \n","    best_valid_loss = float('inf')  # 가장 좋은 validation loss를 저장\n","    early_stop_counter = 0  # 카운터\n","    valid_max_accuracy = -1\n","    valid_max_f1 = -1\n","    \n","\n","    for epoch in range(EPOCH):\n","        model, train_loss, train_accuracy,train_f1 = training(model, train_loader, train_dataset, criterion, optimizer, device, epoch, EPOCH)\n","        model, valid_loss, valid_accuracy,valid_f1 = evaluation(model, valid_loader, val_dataset, criterion, device, epoch, EPOCH)\n","        \n","        if(lr_scheduler_name=='CosineAnnealingLR'):\n","          scheduler.step()\n","          now_lr=scheduler.get_last_lr()[0]\n","        elif(lr_scheduler_name=='ReduceLROnPlateau'):\n","          scheduler.step(valid_loss) # ReduceLROnPlateau는 안에 모니터링할 value를 넣어줘야함. 이전에 scheduler을 선언할때 min,max도. \n","          now_lr= optimizer.param_groups[0]['lr'] # ReduceLROnPlateau는 .get_last_lr()[0]을 지원하지 않음 \n","        else: now_lr = scheduler\n","\n","        if valid_accuracy > valid_max_accuracy:  valid_max_accuracy = valid_accuracy\n","        if valid_f1 > valid_max_f1: valid_max_f1 = valid_f1\n","        \n","        if valid_loss < best_valid_loss: # validation loss가 감소하면 모델 저장 및 카운터 리셋\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), f\"{MODEL_PATH}/{EXP_NAME}.pt\")\n","            early_stop_counter = 0\n","        else: early_stop_counter += 1 # validation loss가 증가하거나 같으면 카운터 증가\n"," \n","        print(f\"Epoch [{epoch + 1}/{EPOCH}], Train Accuracy: {train_accuracy:.4f}, Train Loss: {train_loss:.4f},  Train macro F1: {train_f1:.4f} \")\n","        print(f\"Epoch [{epoch + 1}/{EPOCH}], Valid Accuracy: {valid_accuracy:.4f}, Valid Loss: {valid_loss:.4f},  Valid macro F1: {valid_f1:.4f} \")\n","        print(f\"Epoch [{epoch + 1}/{EPOCH}], Learning Rate: {now_lr}\")\n","        print(\"#\"*70)\n"," \n","        # earlystopping\n","        if early_stop_counter >= patience:\n","            print(\"Early stopping\")\n","            break\n","          \n","    return model\n","          \n","def inference(model,model_path,device,test_loader,tst_dataset,testmode=0):\n","  model.load_state_dict(torch.load(model_path)) # 모델 불러오기\n","  model = model.to(device)\n","  model.eval()\n","  \n","  total_labels = []\n","  total_preds = []\n","  image_names = []\n","  with torch.no_grad():\n","      for images, labels,names in tqdm(test_loader):\n","          images = images.to(device)\n","          labels = labels.to(device)\n","  \n","          outputs = model(images)\n","          if isinstance(outputs, torch.Tensor): outputs = outputs\n","          else: outputs = outputs.logits\n","          _, predicted = torch.max(outputs.data, 1)\n","  \n","          total_preds.extend(predicted.detach().cpu().tolist())\n","          total_labels.extend(labels.tolist())\n","          image_names.extend(names)\n","          \n","  total_preds = np.array(total_preds)\n","  total_labels = np.array(total_labels)\n","  image_names = np.array(image_names)\n","  total_acc = accuracy_score(total_labels, total_preds) \n","  \n","  precision, recall, f1, _ = precision_recall_fscore_support(total_labels, total_preds, average='macro')\n","  \n","  # sample test용 -> 학습하지 않은 데이터셋 추론 결과 시각화 \n","  if(testmode==False):\n","\n","    print(\"Test model accuracy : \",total_acc) \n","    print(\"Test model macro f1 : \",f1) \n","  \n","  # 최종 추론 결과를 제출형태로 만들기 위한 코드 \n","  meta_df=pd.read_csv('data/meta.csv')\n","  id2label = dict(zip(meta_df['target'], meta_df['class_name']))\n","  \n","  result_df = pd.DataFrame({'ID': image_names,'target': total_labels,'pred': total_preds})\n","  result_df['target'] = result_df['target'].map(id2label)\n","  result_df['pred'] = result_df['pred'].map(id2label)\n","  \n","  if(testmode):result_df.drop(['target'],axis=1,inplace=True)\n","\n","  # inference 결과 반환\n","  return result_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## timm말고 모델 불러올 떄 사용\n","# MODEL_NAME= 'microsoft/dit-base-finetuned-rvlcdip'\n","# image_processor  = AutoImageProcessor.from_pretrained(MODEL_NAME)\n","# model = AutoModelForImageClassification.from_pretrained(MODEL_NAME, \n","#     label2id=label2id,\n","#     id2label=id2label,\n","#     ignore_mismatched_sizes = True, \n","#     num_labels=17\n","# ).to(device)\n","# if \"height\" in image_processor.size:\n","#     IMG_SIZE = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n","#     crop_size = size\n","#     max_size = None\n","# elif \"shortest_edge\" in image_processor.size:\n","#     IMG_SIZE = image_processor.size[\"shortest_edge\"]\n","#     crop_size = (size, size)\n","#     max_size = image_processor.size.get(\"longest_edge\")"]},{"cell_type":"markdown","metadata":{},"source":["# Setting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EXP_NAME = \"test\"\n","\n","model_name = \"resnet50\"\n","# resnet50, resnet101.a1_in1k, resnet34, vgg16, beitv2_base_patch16_224.in1k_ft_in22k_in1k, swin_small_patch4_window7_224.ms_in22k_ft_in1k, convnext_small.fb_in22k\n","\n","batch_size = 512\n","\n","image_size = 32\n","\n","lr_scheduler_name=False # 'ReduceLROnPlateau' / 'CosineAnnealingLR'\n","\n","LOSS_F='CE' #  CE = Cross Entropy / Focal = Focal loss\n","\n","weight_decay = False # 1e-3\n","\n","aug_train = True\n","\n","init_learning_rate = 1e-4\n","\n","epoch = 1\n","\n","patience = 5\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n","num_workers = 0\n","\n","MODEL_PATH=\"test_model\"\n","SUB_PATH=\"test_submission\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["# Train & Inference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# timm에서 지정된 모델을 불러옴 \n","model = timm.create_model(model_name,pretrained=True,num_classes=17).to(device)\n","model_config= timm.data.resolve_model_data_config(model)\n","\n","# batchsize와 image size에 따라 transform, dataset, loader을 불러옴 \n","trn_transform,tst_transform=image_trasform(image_size,model_config['mean'],model_config['std'])\n","origin_train_dataset,trn_dataset,val_dataset,tst_dataset,origin_tst_dataset = dataset(aug_train,trn_transform,tst_transform)\n","origin_train_loader,train_loader,valid_loader,test_loader,origin_test_loader = loader(batch_size,origin_train_dataset,trn_dataset,val_dataset,tst_dataset,origin_tst_dataset)\n","\n","# cross entropy or focal loss -> mixup할 경우 training에서 따로 적용\n","if(LOSS_F=='CE'): \n","    loss_fn=nn.CrossEntropyLoss()\n","else:\n","    loss_fn = FocalLoss(gamma=1.5)\n","    m = torch.nn.Softmax(dim=-1)\n","\n","# Weight decay 적용\n","if(weight_decay): optimizer = Adam(model.parameters(), lr=init_learning_rate, weight_decay=weight_decay) \n","else: optimizer = Adam(model.parameters(), lr=init_learning_rate)\n","\n","# learning rate scheduler 적용 -> Cosine Annealing / ReduceLROnPlateau / X\n","if(lr_scheduler_name=='CosineAnnealingLR'): scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n","elif(lr_scheduler_name=='ReduceLROnPlateau'):scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n","else: scheduler = init_learning_rate\n","\n","# train set에서 분리한 train, valid set\n","model = training_loop(model, origin_train_loader, valid_loader, trn_dataset, val_dataset, loss_fn, optimizer, scheduler, device, patience, epoch, lr_scheduler_name)\n","\n","# 학습하지 않은 평가용 sample test\n","result_df=inference(model,f\"{MODEL_PATH}/{EXP_NAME}.pt\",device,test_loader,tst_dataset,testmode=False)\n","\n","# 실제 test set \n","submission=inference(model,f\"{MODEL_PATH}/{EXP_NAME}.pt\",device,origin_test_loader,origin_tst_dataset,testmode=True)\n","\n","sample_submission=pd.read_csv('data/sample_submission.csv')\n","submission = sample_submission.merge(submission, on='ID', how='left')\n","submission.drop(['target'],axis=1,inplace=True)\n","submission.columns=['ID','target']\n","submission['target'] = submission['target'].map(label2id)\n","submission.to_csv(f\"{SUB_PATH}/{EXP_NAME}.csv\",index=False)\n","del model\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_random_samples(n, compare_df,is_correct):\n","    total_len=len(compare_df)\n","    # 맞은 이미지 출력\n","    if(is_correct==1):\n","        df=compare_df[compare_df['target']==compare_df['pred']]\n","        title=\"Correct predict\"\n","        prefix='train'\n","    # 틀린 이미지 출력\n","    elif(is_correct==2):\n","        df=compare_df[compare_df['target']!=compare_df['pred']]\n","        title=\"Wrong predict\"\n","        prefix='train'\n","    # test set 예측 결과 출력\n","    else:\n","        df=compare_df.copy()\n","        df['pred']=''\n","        title='Final result'\n","        prefix='test'\n","    random_indices = np.random.choice(len(df), n, replace=False)\n","    selected_rows = df.iloc[random_indices]\n","\n","    rows = (n + 2) // 3  \n","    cols = min(n, 3)\n","    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n","\n","    for i, (_, row) in enumerate(selected_rows.iterrows()):\n","        ax = axes[i // 3, i % 3] if n > 1 else axes  \n","        image_path = f\"data/aug_train/{row['ID']}\"  \n","        image = cv2.imread(image_path)\n","        ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n","        ax.set_title(f\"ID: {row['ID']} \\n Real: {row['target']} \\n Pred: {row['pred']}\")\n","        ax.axis('off')\n","    for i in range(n, rows * cols):\n","        axes[i // 3, i % 3].axis('off')\n","    plt.suptitle(f\"{title}:{len(df)}/{total_len}\")\n","    plt.show()\n","display_random_samples(9, result_df,1);"]},{"cell_type":"markdown","metadata":{},"source":["# Ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## 이미지 사이즈에 따른 Transform --> 추후에 정규화도 각 모델별로 정의\n","## 영천님 제목 잘라 붙이는 경우\n","# def tta_trasform(image_size,model_mean,model_std):\n","\n","#     tst_transform = A.Compose([\n","#         A.LongestMaxSize(max_size=640, interpolation=cv2.INTER_CUBIC),\n","#         A.PadIfNeeded(min_height=640, min_width=640, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n","#         A.ShiftScaleRotate(shift_limit_x=0.2, shift_limit_y=(0.0, 0.1), scale_limit=0.2, rotate_limit=0, p=0.7, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255]),\n","#         A.Crop (x_min=128, y_min=0, x_max=128+384, y_max=384, p=1.0),\n","#         A.Normalize(mean=list(model_mean), std=list(model_std)),\n","#         ToTensorV2(),\n","#     ])\n","    \n","#     return tst_transform\n","\n","def tta_trasform(image_size,model_mean,model_std):\n","\n","    tst_transform = A.Compose([\n","        A.Resize(height=image_size, width=image_size),\n","        A.Normalize(mean=list(model_mean), std=list(model_std)),\n","        ToTensorV2(),\n","    ])\n","    \n","    return tst_transform"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tta_dataset(AUG_BOOL,tst_transform):\n","\n","    origin_tst_dataset = ImageDataset(\n","        \"data/sample_submission.csv\",\n","        #\"data/test_rot_catformer01/\",\n","        \"data/test/\"\n","        transform=tst_transform\n","    )\n","    \n","    return origin_tst_dataset\n","\n","def tta_loader(batch_size,origin_tst_dataset):\n","    \n","    origin_test_loader = DataLoader(\n","        origin_tst_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=0,\n","        pin_memory=True,\n","        drop_last=False\n","    )\n","    \n","    return origin_test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["meta_df=pd.read_csv('data/meta_kr.csv',encoding='cp949')\n","id2label = dict(zip(meta_df['target'], meta_df['kr']))\n","label2id = dict(zip(meta_df['kr'], meta_df['target']))\n","\n","sample_submission=pd.read_csv('data/sample_submission.csv')\n","sample_submission.drop(['target'],axis=1,inplace=True)\n","\n","image_size=384\n","batch_size=64\n","\n","tst_transform=tta_trasform(image_size,(0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n","origin_tst_dataset = tta_dataset(True,tst_transform)\n","origin_test_loader = tta_loader(batch_size,origin_tst_dataset)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   "]},{"cell_type":"markdown","metadata":{},"source":["## TTA: Test Time Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ttach를 이용하여 augmentation하면 모든 transform 경우의 수가 적용됨.\n","# https://github.com/qubvel/ttach?tab=readme-ov-file\n","\n","import ttach as tta\n","\n","transforms = tta.Compose(\n","    [\n","        tta.HorizontalFlip(),\n","        tta.VerticalFlip(),\n","        #tta.Rotate90(angles = [0, 90, 180]),\n","        #tta.FiveCrops(224, 224),  # 다섯 가지 다른 crop 생성\n","        #tta.Multiply(factors=[0.8, 1, 1.1]),        \n","    ]\n",")\n","\n","## tta의 augmentation을 했을 때 예시 이미지\n","# image = np.array(Image.open('data/test/0b8426f6b3d9d4a3.jpg')) / 255\n","# image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).to(torch.float32)\n","\n","# fig = plt.figure(figsize=(20, 20))\n","# columns = 2\n","# rows = 5\n","\n","# for i, transform in enumerate(transforms):\n","#     image_transformed = transform.augment_image(image)\n","#     image_transformed = np.array(image_transformed.squeeze()).transpose(1, 2, 0)\n","#     fig.add_subplot(rows, columns, i+1)\n","#     plt.imshow(image_transformed)\n","\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def only_inference(model, test_loader, device):\n","    \n","    model.to(device)\n","    model.eval()\n","    probabilities=[]\n","    total_preds = []\n","    image_names = []\n","    with torch.no_grad():\n","        for images, labels, names in tqdm(test_loader):\n","            images = images.to(device)\n","            \n","            outputs = model(images)\n","            probs, predicted = torch.max(outputs.data, 1)\n","\n","            total_preds.extend(predicted.detach().cpu().tolist())\n","            probabilities.extend(probs.detach().cpu().tolist())\n","            image_names.extend(names)\n","            \n","    return image_names,total_preds,probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#model_name='resnet50'\n","model_name=\"caformer_s18.sail_in22k_ft_in1k_384\"\n","            \n","model = timm.create_model(model_name,pretrained=True,num_classes=17)\n","model_config= timm.data.resolve_model_data_config(model)\n","trn_transform,tst_transform=image_trasform(model_config['input_size'][1],model_config['mean'],model_config['std'])\n","\n","#model_path = 'model/Best_resnet50(is=384,bs=64,LS=Red_f1,Shuffle).pt'\n","model_path = 'model/caformer_s18_sail_in22k_ft_in1k_384_loss_titlecrop02.pth'\n","\n","model.load_state_dict(torch.load(model_path)) \n","\n","# ttach에 있는 class로 wrapp해주면, 모든 aug경우의 수 중 merge_mode에 따른 값을 반환\n","# https://github.com/qubvel/ttach/blob/master/ttach/wrappers.py#L52\n","\n","tta_model = tta.ClassificationTTAWrapper(model, transforms, merge_mode='mean')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 선택 모델 tta 추론 결과\n","image_names,total_preds,probabilities = only_inference(tta_model,origin_test_loader,device)\n","result_tta = pd.DataFrame({'ID': image_names,'target': total_preds,'probs':probabilities})\n","\n","# 선택 모델 기본 추론 결과\n","image_names,total_preds,probabilities = only_inference(model,origin_test_loader,device)\n","result_best = pd.DataFrame({'ID': image_names,'target': total_preds,'probs':probabilities})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tta_result = sample_submission.merge(result_tta, on='ID', how='left')\n","tta_result.columns=['ID','tta_target','tta_probs']\n","\n","tta_result = tta_result.merge(result_best, on='ID', how='left')\n","tta_result.columns=['ID','tta_target','tta_probs','bestscore_target','bestscore_probs']\n","tta_result['tta_target']=tta_result['tta_target'].map(id2label)\n","tta_result['bestscore_target']=tta_result['bestscore_target'].map(id2label)\n","tta_result.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 두 모델(tta적용,미적용) 결과의 클래스의 확률값을 비교하여 더 높은 클래스의 확률값을 선정\n","tta_result['final_target'] = tta_result.apply(lambda x: x['tta_target'] if x['tta_probs'] > x['bestscore_probs'] else x['bestscore_target'], axis=1)\n","tta_result[tta_result['tta_target']!=tta_result['bestscore_target']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tta_submission= tta_result[['ID','final_target']]\n","tta_submission.columns=['ID','target']\n","tta_submission['target'] = submission['target'].map(label2id)\n","#submission.to_csv('tta_영천_crop제목.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["break"]},{"cell_type":"markdown","metadata":{},"source":["## Weighted soft voting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_path):\n","    print(model_path)\n","    model_path='best_model/'+model_path\n","    if(model_path in [\"best_model/Final_resnet50(is=224,bs=128,LS=Red_f1,Shuffle).pt\",\"best_model/Re_Final_resnet50(is=224,bs=128,LS=Red_f1,Shuffle).pt\"]):\n","        model = timm.create_model('resnet50',pretrained=True,num_classes=17).to(device)\n","    else:\n","        model = timm.create_model('caformer_s18.sail_in22k_ft_in1k_384',num_classes=17).to(device)\n","    \n","    model.load_state_dict(torch.load(model_path)) \n","    model.eval()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ensemble_inference(models, weights, test_loader):\n","    predictions = torch.zeros(0, dtype=torch.long).to(device)\n","    image_names = []\n","    probabilities=[]\n","    with torch.no_grad():\n","        for images, labels, names in tqdm(test_loader):\n","            images = images.to(device)\n","            avg_output = torch.zeros(images.size(0), 17).to(device)\n","            \n","            for model, weight in zip(models, weights):\n","                # metaformer은 특정 모델 class 이름-> inputsize 384 \n","                if model.__class__.__name__ in ['MetaFormer']:\n","                    preprocess = transforms.Compose([\n","                        transforms.Resize((384, 384)),\n","                    ])\n","                    images = preprocess(images)\n","                outputs = model(images)\n","                avg_output += weight * F.softmax(outputs, dim=1)\n","            probs, preds = torch.max(avg_output, 1)\n","            \n","            predictions = torch.cat((predictions, preds), dim=0)\n","            probabilities.extend(probs.detach().cpu().tolist())\n","            image_names.extend(names)\n","            \n","        predictions=predictions.cpu().numpy()\n","            \n","    return predictions,image_names,probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_paths = [\"Final_resnet50(is=224,bs=128,LS=Red_f1,Shuffle).pt\",'caformer_s18_sail_in22k_ft_in1k_384_loss_titlecrop.pth','caformer_s18_sail_in22k_ft_in1k_384.pth'] \n","models_ = [load_model(path) for path in model_paths]\n","weights = [0.1,0.4,0.4] # 각 모델별 가중치 \n","\n","## tta모델도 추가할 경우\n","#tta_model.eval()\n","#tta_model.to(device)\n","#models_.append(tta_model)\n","#weights.append(0.1)\n","\n","ensemble_predictions,image_names,probabilities = ensemble_inference(models, weights, origin_test_loader)\n","\n","result_weighted = pd.DataFrame({'ID': image_names,'target': ensemble_predictions,'probs':probabilities})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weight_result = sample_submission.merge(result_tta, on='ID', how='left')\n","weight_result.columns=['ID','weight_target','weight_probs']\n","\n","weight_result = weight_result.merge(result_best, on='ID', how='left')\n","weight_result.columns=['ID','weight_target','weight_probs','bestscore_target','bestscore_probs']\n","weight_result['weight_target'] = weight_result['weight_target'].map(id2label)\n","weight_result['bestscore_target'] = weight_result['bestscore_target'].map(id2label)\n","weight_result.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weight_result[(weight_result['weight_probs']>weight_result['bestscore_probs']) & (weight_result['weight_target']!=weight_result['bestscore_target'])]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weight_result['final_target'] = weight_result.apply(lambda x: x['weight_target'] if x['weight_probs'] > x['bestscore_probs'] else x['bestscore_target'], axis=1)\n","weight_result[weight_result['weight_target']!=weight_result['bestscore_target']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["weight_result= weight_result[['ID','final_target']]\n","weight_result.columns=['ID','target']\n","weight_result['target'] = weight_result['target'].map(label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#weight_result.to_csv(\"submission/영천_창현_weigthed+probs.csv\",index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Hard Voting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["folder_path = \"best_submission/\"\n","\n","file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n","\n","label_encoder = LabelEncoder()\n","encoded_labels = label_encoder.fit_transform(file_list)\n","\n","label_mapping = dict(zip(encoded_labels, file_list))\n","\n","dfs = []\n","for label, file in label_mapping.items():\n","    file_path = os.path.join(folder_path, file)\n","    df = pd.read_csv(file_path)\n","    df = df.rename(columns={'target': f'target_{label}'})\n","    dfs.append(df)\n","\n","voting_result = pd.concat(dfs, axis=1, join='inner')\n","voting_result = voting_result.loc[:,~voting_result.columns.duplicated()]\n","voting_result.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["voting_result['mode_TARGET'] = voting_result.mode(axis=1)[0]\n","voting_result.iloc[:, 1:] = voting_result.iloc[:, 1:].apply(lambda col: col.map(id2label))\n","voting_result.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_mapping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#diff = voting_result[(voting_result[['target_0', 'target_2', 'target_1','target_3']].values != voting_result['mode_TARGET'].values.reshape(-1, 1)).any(axis=1)]\n","diff = voting_result[(voting_result[['target_3']].values != voting_result['mode_TARGET'].values.reshape(-1, 1)).any(axis=1)]\n","len(diff)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 모두 한번씩 나와서 최빈값이 적용이 안될 경우\n","nan_rows = voting_result[voting_result['mode_TARGET'].isna()]\n","nan_rows"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 가장 리더보드 스코어가 좋은 결과값으로 채움\n","voting_result.loc[nan_rows.index, 'mode_TARGET'] = nan_rows['target_3']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["submission = voting_result[['ID','mode_TARGET']]\n","submission.columns=['ID','target']\n","submission['target']=submission['target'].map(label2id)\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#submission.to_csv(\"영천096+영천crop_tta창현+예람pred21+09532.csv\",index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
